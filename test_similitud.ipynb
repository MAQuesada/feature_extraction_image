{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- DENSE -- ORB --------\n",
      "Número de puntos clave en la primera imagen: 2701\n",
      "Número de puntos clave en la segunda imagen: 3796\n",
      "Número de matches en total: 2701\n",
      "Número de matches: 10\n",
      "________________________________________\n",
      "------- FAST -- ORB --------\n",
      "Número de puntos clave en la primera imagen: 2459\n",
      "Número de puntos clave en la segunda imagen: 1976\n",
      "Número de matches en total: 2459\n",
      "Número de matches: 2\n",
      "________________________________________\n",
      "------- GFTT -- ORB --------\n",
      "Número de puntos clave en la primera imagen: 1471\n",
      "Número de puntos clave en la segunda imagen: 967\n",
      "Número de matches en total: 1471\n",
      "Número de matches: 0\n",
      "________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Empezamos importando las librerías necesarias.\n",
    "import itertools\n",
    "import cv2\n",
    "import numpy as np\n",
    "from imutils.feature import FeatureDetector_create, \\\n",
    "    DescriptorExtractor_create, DescriptorMatcher_create\n",
    "\n",
    "# Como parámetros de entrada podemos especificar:\n",
    "#  1. Las imágenes a ser comparadas.\n",
    "#  2. El detector de puntos clave a usar.\n",
    "#  3. El descriptor de features que vamos a usar.\n",
    "#  4. La función de similitud o \"feature matcher\" que aplicaremos.\n",
    "#  5. Un flag para indicar si queremos visualizar los resultados o no.\n",
    "\n",
    "\n",
    "# detectores de keypoint\n",
    "all_keypoints = [\"DENSE\", \"FAST\", \"GFTT\"]\n",
    "#[\"BRISK\", \"DENSE\", \"FAST\", \"GFTT\", \"HARRIS\", \"MSER\", \"ORB\"]\n",
    "\n",
    "# all_keypoints = [\"DOG\",\"SIFT\",\"SURF\",]\n",
    "\n",
    "\n",
    "all_descriptors = [\"ORB\"]\n",
    "\n",
    "\n",
    "all_matchers = [\"BruteForce\", \"BruteForce-Hamming\", \"FlannBased\"]\n",
    "# all_matchers = [\"FlannBased\"]\n",
    "\n",
    "\n",
    "for kp, desc in itertools.product(all_keypoints, all_descriptors):\n",
    "\n",
    "    print('-------', kp, '--', desc, '--------')\n",
    "    # instanciamos el detector de puntos claves\n",
    "    detector = FeatureDetector_create(kp)\n",
    "\n",
    "    # Instanciamos el extractor/descriptor\n",
    "    extractor = DescriptorExtractor_create(desc)\n",
    "\n",
    "    # Instanciamos el \"feature matcher\"\n",
    "\n",
    "    # matcher = DescriptorMatcher_create(match)\n",
    "    matcher = cv2.BFMatcher()\n",
    "\n",
    "    # Cargamos en memoria las imágenes a ser comparadas.\n",
    "    image1 = cv2.imread('1173878dup/18814714.jpg')\n",
    "    # image2 = cv2.imread('1173878dup/18814718.jpg')\n",
    "    image2 = cv2.imread('1173903dup/18815391.jpg')\n",
    "\n",
    "    # Luego, convertimos ambas imágenes a escala de grises.\n",
    "    first_gray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "    second_gray = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculamos los puntos clave de las dos imágenes.\n",
    "    first_keypoints = detector.detect(first_gray)\n",
    "    second_keypoints = detector.detect(second_gray)\n",
    "\n",
    "    # Calculamos los vectores descriptivos de ambas imágenes\n",
    "    first_keypoints, first_features = extractor.compute(first_gray,\n",
    "                                                        first_keypoints)\n",
    "    second_keypoints, second_features = extractor.compute(second_gray,\n",
    "                                                          second_keypoints)\n",
    "\n",
    "    # Utilizamos KNN para llevar a cabo la vinculación de los\n",
    "    # vectores de la primera imagen con los de la segunda. El último\n",
    "    # parámetro indica que queremos los dos vectores más cercanos\n",
    "    # por cada par.\n",
    "    raw_matches = matcher.knnMatch(first_features, second_features, 2)\n",
    "    matches = []\n",
    "\n",
    "    for match in raw_matches:\n",
    "        # Tenemos que verificar que cada match sea válido. Para\n",
    "        # ello, un match debe tener exactamente dos elementos\n",
    "        # para poder aplicar el test de Lowe, el cual rechaza\n",
    "        # aquellos matches donde la distancia entre los dos\n",
    "        # mejores vectores (match[0] y match[1]) esté por encima\n",
    "        # de 0.8.\n",
    "        if (len(match) == 2 and\n",
    "                match[0].distance < match[1].distance * 0.55):\n",
    "            # Si pasamos la prueba, nos quedamos con el mejor\n",
    "            # match, match[0]\n",
    "            matches.append((match[0].trainIdx,\n",
    "                            match[0].queryIdx))\n",
    "    # Imprimimos el número de puntos clave de la imagen 1, la\n",
    "    # imagen 2, así como el número de matches entre ambas.\n",
    "    print(f'Número de puntos clave en la primera imagen: '\n",
    "          f'{len(first_keypoints)}')\n",
    "    print(f'Número de puntos clave en la segunda imagen: '\n",
    "          f'{len(second_keypoints)}')\n",
    "    print(f'Número de matches en total: {len(raw_matches)}')\n",
    "    print(f'Número de matches: {len(matches)}')\n",
    "\n",
    "    # Ahora es momento de visualizar. Empezamos extrayendo las\n",
    "    # dimensiones de ambas imágenes.\n",
    "    first_height, first_width = image1.shape[:2]\n",
    "    second_height, second_width = image2.shape[:2]\n",
    "    # Creamos un lienzo sobre el que dibujaremos ambas imágenes\n",
    "    # junto con líneas que unan los matches entre ellas.\n",
    "    visualization = np.zeros((max(first_height, second_height),\n",
    "                              first_width + second_width, 3),\n",
    "                             dtype='uint8')\n",
    "    # Añadimos la primera imagen al lado izquierdo de la\n",
    "    # visualización.\n",
    "    visualization[:first_height, :first_width] = image1\n",
    "    # Añadimos la segunda imagen al lado derecho de la\n",
    "    # visualización.\n",
    "    visualization[:second_height, first_width:] = image2\n",
    "    # El ciclo de abajo lo que hace es iterar por cada match,\n",
    "    # dibujando una línea que parta de la primera imagen,\n",
    "    # unbicada a la izquierda, y que termine en la segunda,\n",
    "    # localizada en la parte derecha.\n",
    "    for train_index, query_index in matches:\n",
    "        # Escogemos un color aleatorio para la línea.\n",
    "        color = np.random.randint(0, high=255, size=(3,))\n",
    "        color = tuple(map(int, color))\n",
    "        # Calculamos el primer extremo de la línea.\n",
    "        first_point = (int(first_keypoints[query_index].pt[0]),\n",
    "                       int(first_keypoints[query_index].pt[1]))\n",
    "        # Calculamos el segundo extremo de la línea.\n",
    "        second_point = (int(second_keypoints[train_index].pt[0]\n",
    "                            + first_width),\n",
    "                        int(second_keypoints[train_index].pt[1]))\n",
    "        # Dibujamos la línea.\n",
    "        cv2.line(visualization, first_point, second_point,\n",
    "                 color, 2)\n",
    "    cv2.imshow('Matched', visualization)\n",
    "    cv2.waitKey(0)\n",
    "    print('________________________________________')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
